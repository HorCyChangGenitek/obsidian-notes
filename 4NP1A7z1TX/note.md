# Your local LLM is 10x slower than it should be

---
title: "Your local LLM is 10x slower than it should be"
source: "https://www.youtube.com/watch?v=L9QZ97y9Exg"
author:
  - "[[@SandTiger42]]"
published: 2026-02-01
created: 2026-02-04
description: "Here’s the one change that took mine from ~120 tok/s to 1,200+ without a new GPU.TryHackMe just launched Cyber Security 101 (SEC1) — and for a limited time y..."
tags:
  - "clippings"
---
![](https://www.youtube.com/watch?v=L9QZ97y9Exg)

這段影片標題為「你的本地大型語言模型（LLM）比應有的速度慢了 10 倍」，由 Alex Ziskind 製作。影片主要探討如何透過優化配置與架構，大幅提升本地 LLM 的推論吞吐量（Throughput）。

以下是影片的核心重點與技術細節：

### 1. 本地 LLM 的速度瓶頸

- **常見現況**：大多數使用者使用 Ollama 或 Llama.cpp 進行對話，通常僅能達到約 100-120 Tokens/s 的速度 [00:10](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=10)。
    
- **問題點**：當進行單一對話時，這種速度尚可接受；但若作為代碼助手（Code Assistant）或驅動多個 AI 代理（Agents）時，單一請求的模式會成為嚴重瓶頸 [01:12](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=72)。
    

### 2. 提升性能的關鍵技術

影片展示了如何將速度從 100 提升至 **1,200+ Tokens/s** 的技巧：

- **多實例執行（Multiple Instances）**：同時啟動多個 Llama Server 實例，充分利用 GPU 的運算能力 [05:17](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=317)。
    
- **調整參數**：透過調整 Llama.cpp 的 `parallel`（並行數）與 `concurrency`（併發數）等參數，優化任務分配 [06:12](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=372)。
    
- **負載平衡（Load Balancing）**：在多個 Llama Server 前端架設 **Nginx**，使用 **Round Robin（輪詢）** 機制分配請求，讓多個實例共同處理大量任務 [09:04](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=544)。
    

### 3. Llama Throughput Lab 工具

作者開發了一個開源工具 **Llama Throughput Lab**，具備以下功能 [05:05](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=305)：

- **自動化掃描（Full Sweep）**：自動測試數百種實例數、並行度與併發數的組合，找出該電腦硬體下的最佳配置點 [07:07](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=427)。
    
- **一鍵啟動**：自動配置 Nginx 並啟動多個背景伺服器實例，簡化複雜的手動設定 [08:33](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=513)。
    

### 4. 實測數據與應用

- **硬體參考**：作者在 Mac Studio（具備 512GB 統一記憶體）上進行測試，成功達到 1,226 Tokens/s 的極高吞吐量 [07:58](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=478)。
    
- **適用場景**：這種優化對於需要同時分析數千張圖片、處理長影片影格，或是執行複雜代理工作流的開發者特別有效 [06:46](http://www.youtube.com/watch?v=L9QZ97y9Exg&t=406)。
    

### 來源連結

- [YouTube 影片：Your local LLM is 10x slower than it should be](http://www.youtube.com/watch?v=L9QZ97y9Exg)



## 影片逐字稿
這是一份針對該影片逐字稿的繁體中文（台灣用語修正）逐字翻譯：

---

### 影片逐字翻譯稿

**0:00**

你可能對 Ollama 很熟悉，對吧？這就是它的樣子。你可以啟動它，可以跟它對話。先別管我們用的是 Qwen 34B，那只是我慣用的標準配備。

**0:09**

提示詞（Prompt）內容並不重要。Ollama 給我們的是每秒 100 個標記（Tokens）。Ollama 是跑在 llama.cpp 之上的，雖然會有一點額外開銷，但你也可以直接執行 llama.cpp 並進行遠端查詢。

**0:20**

如果是軟體開發人員，想要把它當作程式碼助理或搭配代理人（Agents）使用，這就是他們會感興趣的地方。稍等一下，這等一下會是關鍵。

**0:29**

我要啟動 Llama Server，順便說一下，指向的是同一個模型。Llama Server 有個很漂亮的介面，我們也可以跟它打個招呼。

**0:36**

這個給我們的每秒標記數是 124，好了一點點。那是聊天模式，我們一次處理一個模型、一個請求。但程式碼助理並不是這樣運作的，因為它需要同時處理許多不同的對話。

**0:48**

如果我從這裡遠端查詢呢？我寫了一個小腳本。這基本上來自我的「挑戰一百萬」影片，但這次只做一千個。我要從我的筆記型電腦通訊到執行 Llama Server 的 Mac Studio。

**1:01**

砰！開始了。現在我只能在那邊乾等。砰、砰、砰。你看，每秒 120 個標記。數字非常接近，對吧？

**1:10**

你知道誰不應該乾等嗎？是那些正在處理你程式碼庫的代理人們。有時是幾十個，甚至上百個。我們甚至要把目標標記數增加到 1,000，這樣才有發揮的空間。

**1:24**

砰！我要即時觀察這個。你看，這會很瘋狂。什麼？llama.cpp 跑出每秒 826 個標記！

**1:33**

你可能會想：「喔，好啦，我以前看過這招，大概是在你頻道上看過。」併發（Concurrency）設定為 128。嘿，我正要給你看個新招。

**1:42**

因為即使併發設為 128，如果沒有那個新招，你還是只能得到每秒 231 個標記。雖然已經比較好了，但我們受限於 llama.cpp 本身的能力。它無法像 vLLM 之類的工具那樣處理那麼多併發連線，我有其他影片展示過這點。

**2:03**

那我到底在做什麼？我寫了這個啟動器（Launcher）。這實際上是受 Donato Capitella 的分散式啟動器啟發。我最近在另一支影片介紹過那個東西。

**2:15**

但在這裡，我們只是在單一機器上執行。我建立了這個啟動器，讓你可以調整一些 llama.cpp 提供的參數（Knobs），同時掃描（Sweeping）並探索你機器的極限。

**2:28**

舉例來說，我在 Mac Studio 上跑，但如果你只有 Mac Mini 呢？你也可以在那上面跑。如果你有 Windows 筆電、Linux 筆電或 Nvidia 的機器，都沒關係，因為 llama.cpp 在這些平台上都能跑。

**2:42**

這個啟動器也可以在任何地方執行，因為它只是 Python。它會為你找到執行 Llama Server 的最佳組合，同時調整那些「參數」。我大概不該在 YouTube 上說「扭動那些旋鈕（Twisting those knobs）」，可能會有人投訴。

**2:56**

（音效：FBI，開門！）

**2:59**

某些敏感的靈魂。讓我示範這怎麼運作。首先，我們有一些測試。我們可以做單一請求。選擇你的模型、選擇執行位置，然後執行。

**3:09**

現在，這會查詢 llama.cpp，得到該設定下的每秒標記數，這裡是 127，跟我們之前看到的很接近。你可以做併發請求。來吧。

**3:18**

併發請求，總數 128，併發數 128。顯示總標記數、平均請求標記數，然後是吞吐量（Throughput）。同一個模型，我們得到了每秒 240 個標記的吞吐量。

**3:28**

順便說一下，所有這些東西都記錄在儲存庫（Repository）中。如果你現在正進入資安（Cyber）領域，在「看影片」和「處理真實狀況」之間有一個尷尬的鴻溝。

**3:39**

這個鴻溝正是這個工具的用途。這是 TryHackMe，讓你透過實作學習資安。互動式實驗室、真實場景、全瀏覽器操作，還有超過 600 萬人的全球社群。

**3:52**

他們剛推出了一個入門級認證叫做 Cybersecurity 101，也稱為 SEC1。簡單導覽一下，不爆雷考試內容。核心概念是「應用基礎」。你接受動手實作訓練，然後 SEC1 會檢查你是否能在真實情況下執行基礎操作，而不只是背誦定義。

**4:11**

這是給正在尋求第一份資安工作、轉職者、實習生或 0-2 年經驗的初級人員，想要證明自己具備實作基礎的人。

**4:20**

高層次來說，它涵蓋了你會遇到的東西：作業系統、網路、網頁安全基礎、藍軍（Blue Team）基礎、紅軍（Red Team）基礎，以及初級的惡意軟體分析。

**4:33**

這就是它與眾不同的地方。很多初階認證感覺像單字測驗，SEC1 則是試圖成為一個實作檢查點。完成後，你會立即得到結果、數位證書和可分享的徽章。

**4:46**

如果你正開始學習資安並想要證明基礎實力，請查看下方連結。使用我的折扣碼，非 Premium 用戶可享認證 6 折，外加 3 個月免費 Premium 會籍。

**5:04**

所有東西都在這個儲存庫中，叫做 **Llama Throughput Lab**。這裡還有其他測試，但有一個非常有趣，叫做 **Full Sweep（全面掃描）**。

**5:12**

這需要一段時間。它會測試 308 種不同的組合，包括你要啟動多少個 Llama Server 實體（Instances）。這是關鍵，提示一下。

**5:21**

你執行多個 Llama Server 實體來進行回應，因為像這樣強大的機器現在都有很大的記憶體。這台有 64GB 的 VRAM（Apple Silicon 稱為統一記憶體），這台 Mac Studio 甚至有 512GB。

**5:37**

但執行這些模型時，記憶體不是限制因素，限制因素是 GPU 的算力。我們全部用 GPU 跑，因為不想用 CPU，那會慢得要命。

**5:49**

當你同時執行多個 Llama Server 時，顯然不再受限於記憶體，除非你跑的是巨型模型。在 512GB 的機器上，你可以同時在四個 Llama Server 實體中跑四個 Llama 70B 模型。

**6:10**

所以那是「實體數」。接著，Llama Server 提供了一個叫做 `parallel` 的參數。這在 llama.cpp 的 GitHub 儲存庫上有很好的文件紀錄，由作者 Georgi Gerganov 親自撰寫。這又是另一個你可以調整的「旋鈕」。

**6:26**

好吧，我大概該停止這個隱喻了，有人會生氣的。我沒有在扭任何人的旋鈕。

**6:32**

併發（Concurrency）是另一個可以修改的參數，這就是我之前影片展示過的，我們可以向 Llama Server 發送多個請求。為什麼我們要關心這個？

**6:39**

如果你只是在聊天，那當然就慢慢聊、等它跑完。但很多時候我們想更進一步。想像一個裝有數千張照片的資料夾需要分析，或者一段有數百萬影格（Frames）的影片。你不會想坐在那裡等它一個影格一個影格分析吧？

**6:55**

或者想像另一種情境，你有一群代理人協同運作，有的負責規劃，有的負責執行。你希望他們同時工作。這就是併發派上用場的地方。

**7:06**

回到掃描測試。這會做全面掃描：實體數、平行數和併發數，它會找出你電腦上的最佳甜蜜點。你可以選擇該測試，點擊執行，它就開始了。

**7:21**

請記住這需要時間。在慢一點的系統上會花非常久。現在我們才跑到 308 種組合中的第 4 種。但隨著掃描進行，你可以看到數字跳出來：128、193、263、314。

**7:39**

你可以看到這些不同參數如何影響輸出。如果我進入結果資料夾，我當然在 Mac Studio 上跑過完整掃描了。

**7:47**

我大概應該做個漂亮的圖表，但這在 GitHub 上是開源的，想玩的話儘管去改。看看這些數字：**每秒 1,226 個標記**！

**7:58**

那是用了 16 個實體。換句話說，我啟動了 Llama Server 16 次。`parallel` 旗標設為 64，併發數 1,024。

**8:08**

請記住這只是基準測試（Benchmark）。如果你有實際應用案例，這只能提供引導和方向，請務必實際測試你的真實場景。

**8:17**

最後，討論一下我實際上是怎麼執行的。我是用這個指令：這是一個可執行的 Python 指令 `run-llama-tests`。砰！我知道，沒什麼創意。

**8:26**

在這裡你可以選擇測試、選擇模型。砰！還有一件事，就是這個 6 號選項（目前是 6 號，以後可能會變）：**Configure and run round-robin**。

**8:36**

這就是實際為你啟動多個伺服器的東西。你當然可以手動啟動，但那一點都不好玩。在這裡你可以設定要啟動多少個實體，假設我要啟動 16 個。

**8:50**

Parallelism 是 Llama Server 的參數。這是第一個 Llama Server 的啟動連接埠（Port），之後每個伺服器會自動加 1，所以會一路加到 9115。

**9:01**

最後，這裡有個關鍵：**Nginx**。這是你不該害怕的東西。我以前怕了很久，但它真的很酷，因為它是坐在你的 Llama Servers 前面的小伺服器，你直接跟它對話。

**9:13**

它基本上只是轉發（Pass through），但它有一個很棒的功能叫做「輪詢（Round-robin）」。想像每個實體都在跑，第一個請求進來，送去這裡；第二個進來，送去那裡，以此類推。

**9:26**

它會輪流分配這些請求，這樣就不會全部塞在同一個伺服器上。我們來設定一下。我喜歡用 8000 連接埠，因為好記。

**9:34**

主機位置（Host），本地跑就設 `localhost`，遠端查詢就設 `0.0.0.0`。然後你有啟動和停止輪詢伺服器的選項。我們來試試。

**9:48**

現在看 Mac Studio 的活動監視器，你會看到 16 個快樂的小 Llama Server 正在跑，等待處理請求。

**9:57**

你可能熟悉 Open WebUI，我之前做過一個用它搭配 Ollama 的教學，但 Open WebUI 可以搭配很多不同的伺服器。所以我把它指向我的 Nginx 端點，也就是我們設定的 IP 和 8000 連接埠。

**10:14**

開個新對話。哈囉。砰！它回應了。現在，我沒有特別設定它來處理長故事，所以像「Say hello」這種短提示詞效果很好，因為它不需要思考很久。

**10:25**

但「思考」其實是推論（Inference）的一部分，它在思考時其實也在生成標記，會消耗標記數。我設定了生成標記的上限，你可以根據需要調整。

**10:36**

「法國首府在哪？」、「用西班牙文跟我打招呼」。它們幾乎是同時完成的。所以，去試試看吧！玩玩 **Llama Throughput Lab**，讓我知道你的想法。

**10:47**

你可以開 Issue 或 Pull Request 等等。如果你喜歡這支影片，你大概也會喜歡我拍的關於這個框架（Cluster）的影片，就在這邊。感謝收看，下次見。

---

**下一步建議：**

如果您需要針對影片中提到的特定技術（例如 **@npm** Nginx Proxy Manager 如何實作輪詢）或 **llama.cpp** 的參數進行更深入的技術分析，歡迎隨時告訴我！